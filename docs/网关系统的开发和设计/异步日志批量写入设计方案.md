# 异步日志批量写入设计方案

## 1. 核心设计理念

**异步批量写入**的设计包含以下关键要素：

### 1.1 异步处理架构
```java
// 异步模式：添加到队列，不阻塞业务线程
AuditLogEntry entry = new AuditLogEntry(action, message, System.currentTimeMillis());
boolean offered = logQueue.offer(entry, 100, TimeUnit.MILLISECONDS);

if (offered) {
    asyncLogs.incrementAndGet();
} else {
    // 队列满，降级到同步写入
    syncAuditService.log(action, message);
}
```

### 1.2 批量写入策略
```java
// 定时批量刷新
scheduledExecutor.scheduleAtFixedRate(
    this::flushBatch, 
    flushIntervalMs, 
    flushIntervalMs, 
    TimeUnit.MILLISECONDS
);

// 批次大小触发刷新
if (batch.size() >= batchSize) {
    flushBatch();
}
```

### 1.3 故障容错机制
```java
// 异常时降级到同步写入
.onErrorResume(e -> {
    log.error("Failed to write audit log batch", e);
    for (AuditLogEntry entry : batch) {
        syncAuditService.log(entry.getAction(), entry.getMessage());
    }
});
```

## 2. 设计优势

### 2.1 性能优势
- **非阻塞处理**：业务线程不等待日志写入完成
- **批量I/O**：减少磁盘写入次数，提升吞吐量
- **内存缓冲**：使用内存队列缓存，减少磁盘压力
- **并发处理**：独立的异步线程处理日志写入

### 2.2 可靠性优势
- **故障降级**：队列满或异常时自动降级到同步写入
- **数据不丢失**：确保所有日志都能被记录
- **优雅关闭**：服务关闭时刷新剩余日志
- **监控完善**：提供详细的统计信息和健康检查

### 2.3 可配置性
- **灵活配置**：支持批次大小、刷新间隔、队列大小等参数
- **多种存储**：支持文件、数据库、Elasticsearch、Kafka等存储方式
- **动态调整**：运行时可以调整配置参数

## 3. 实现细节

### 3.1 核心组件
1. **AsyncAuditService**：异步审计日志服务主类
2. **AuditConfig**：配置管理类
3. **AuditController**：监控和管理接口
4. **AuditLogEntry**：日志条目封装类

### 3.2 线程模型
```java
// 异步处理线程
private final ExecutorService asyncExecutor = Executors.newSingleThreadExecutor();

// 定时刷新线程
private final ScheduledExecutorService scheduledExecutor = Executors.newSingleThreadScheduledExecutor();

// 日志队列
private final BlockingQueue<AuditLogEntry> logQueue;
```

### 3.3 批量写入流程
1. **日志入队**：业务线程将日志添加到队列
2. **异步处理**：独立线程从队列获取日志
3. **批次累积**：将日志累积到内存批次中
4. **定时刷新**：定期或达到批次大小时批量写入
5. **故障处理**：异常时降级到同步写入

## 4. 配置参数

### 4.1 异步配置
```yaml
audit:
  async:
    enabled: true          # 是否启用异步模式
    batch-size: 100        # 批次大小
    flush-interval: 5000   # 刷新间隔（毫秒）
    queue-size: 10000      # 队列大小
    queue-timeout: 100     # 队列超时时间（毫秒）
    worker-threads: 1      # 工作线程数
```

### 4.2 存储配置
```yaml
audit:
  storage:
    type: file             # 存储类型：file, database, elasticsearch, kafka
    file:
      path: logs/audit.log # 日志文件路径
      rotation-enabled: true
      max-file-size: 100   # 单个文件最大大小（MB）
      max-files: 10        # 保留文件数量
```

## 5. 监控和管理

### 5.1 统计信息
- **totalLogs**：总日志数
- **asyncLogs**：异步处理日志数
- **syncLogs**：同步处理日志数
- **batchWrites**：批量写入次数
- **failedWrites**：失败写入次数
- **queueSize**：当前队列大小
- **currentBatchSize**：当前批次大小

### 5.2 管理接口
- `GET /api/gateway/audit/stats`：获取统计信息
- `POST /api/gateway/audit/flush`：手动刷新批次
- `POST /api/gateway/audit/test`：测试日志记录
- `GET /api/gateway/audit/health`：健康检查
- `GET /api/gateway/audit/config`：获取配置信息

## 6. 使用场景

### 6.1 高并发场景
- 网关API调用审计
- 用户操作日志记录
- 系统事件追踪

### 6.2 性能敏感场景
- 对响应时间要求极高的API
- 大量并发请求处理
- 实时数据处理

### 6.3 存储优化场景
- 减少磁盘I/O压力
- 优化存储成本
- 提升写入效率

## 7. 扩展性

### 7.1 存储扩展
- **文件存储**：适合本地日志收集
- **数据库存储**：适合结构化查询
- **Elasticsearch**：适合全文搜索和分析
- **Kafka**：适合流式处理和实时分析

### 7.2 功能扩展
- **日志压缩**：减少存储空间
- **日志加密**：保护敏感信息
- **日志过滤**：按条件过滤日志
- **日志聚合**：统计分析功能

## 8. 实现代码示例

### 8.1 异步审计服务核心实现
```java
@Slf4j
@Service
public class AsyncAuditService implements AuditService {

    @Value("${audit.async.batch-size:100}")
    private int batchSize;

    @Value("${audit.async.flush-interval:5000}")
    private long flushIntervalMs;

    @Value("${audit.async.queue-size:10000}")
    private int queueSize;

    @Value("${audit.async.enabled:true}")
    private boolean asyncEnabled;

    // 异步处理线程池
    private final ExecutorService asyncExecutor = Executors.newSingleThreadExecutor(r -> {
        Thread t = new Thread(r, "async-audit-worker");
        t.setDaemon(true);
        return t;
    });

    // 定时刷新线程池
    private final ScheduledExecutorService scheduledExecutor = Executors.newSingleThreadScheduledExecutor(r -> {
        Thread t = new Thread(r, "async-audit-scheduler");
        t.setDaemon(true);
        return t;
    });

    // 日志队列
    private final BlockingQueue<AuditLogEntry> logQueue;
    
    // 当前批次缓存
    private final AtomicReference<List<AuditLogEntry>> currentBatch = new AtomicReference<>(new ArrayList<>());
    
    // 统计信息
    private final AtomicLong totalLogs = new AtomicLong(0);
    private final AtomicLong batchWrites = new AtomicLong(0);
    private final AtomicLong failedWrites = new AtomicLong(0);
    private final AtomicLong asyncLogs = new AtomicLong(0);
    private final AtomicLong syncLogs = new AtomicLong(0);

    // 同步审计服务（降级使用）
    private final SyncAuditService syncAuditService = new SyncAuditService();

    public AsyncAuditService() {
        this.logQueue = new LinkedBlockingQueue<>(queueSize);
    }

    @PostConstruct
    public void init() {
        if (asyncEnabled) {
            // 启动异步处理线程
            asyncExecutor.submit(this::processLogsAsync);
            
            // 启动定时刷新任务
            scheduledExecutor.scheduleAtFixedRate(
                this::flushBatch, 
                flushIntervalMs, 
                flushIntervalMs, 
                TimeUnit.MILLISECONDS
            );
            
            log.info("Async audit service initialized - batchSize: {}, flushInterval: {}ms, queueSize: {}", 
                    batchSize, flushIntervalMs, queueSize);
        } else {
            log.info("Async audit service disabled, using sync mode");
        }
    }

    @PreDestroy
    public void shutdown() {
        log.info("Shutting down async audit service...");
        
        // 刷新剩余日志
        flushBatch();
        
        // 关闭线程池
        asyncExecutor.shutdown();
        scheduledExecutor.shutdown();
        
        try {
            if (!asyncExecutor.awaitTermination(5, TimeUnit.SECONDS)) {
                asyncExecutor.shutdownNow();
            }
            if (!scheduledExecutor.awaitTermination(5, TimeUnit.SECONDS)) {
                scheduledExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            asyncExecutor.shutdownNow();
            scheduledExecutor.shutdownNow();
        }
        
        log.info("Async audit service shutdown completed");
    }

    @Override
    public void log(String action, String message) {
        totalLogs.incrementAndGet();
        
        if (!asyncEnabled) {
            // 同步模式
            syncLogs.incrementAndGet();
            syncAuditService.log(action, message);
            return;
        }

        try {
            // 异步模式：添加到队列
            AuditLogEntry entry = new AuditLogEntry(action, message, System.currentTimeMillis());
            boolean offered = logQueue.offer(entry, 100, TimeUnit.MILLISECONDS);
            
            if (offered) {
                asyncLogs.incrementAndGet();
                log.debug("Audit log queued: {} - {}", action, message);
            } else {
                // 队列满，降级到同步写入
                syncLogs.incrementAndGet();
                log.warn("Audit queue full, falling back to sync write: {} - {}", action, message);
                syncAuditService.log(action, message);
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            // 中断时降级到同步写入
            syncLogs.incrementAndGet();
            syncAuditService.log(action, message);
        } catch (Exception e) {
            // 异常时降级到同步写入
            syncLogs.incrementAndGet();
            log.error("Failed to queue audit log, falling back to sync write", e);
            syncAuditService.log(action, message);
        }
    }

    /**
     * 异步处理日志
     */
    private void processLogsAsync() {
        log.info("Async audit log processor started");
        
        while (!Thread.currentThread().isInterrupted()) {
            try {
                // 从队列获取日志条目
                AuditLogEntry entry = logQueue.poll(1, TimeUnit.SECONDS);
                if (entry != null) {
                    // 添加到当前批次
                    List<AuditLogEntry> batch = currentBatch.get();
                    batch.add(entry);
                    
                    // 检查是否需要刷新批次
                    if (batch.size() >= batchSize) {
                        flushBatch();
                    }
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            } catch (Exception e) {
                log.error("Error in async audit log processor", e);
            }
        }
        
        log.info("Async audit log processor stopped");
    }

    /**
     * 刷新批次
     */
    private void flushBatch() {
        List<AuditLogEntry> batch = currentBatch.getAndSet(new ArrayList<>());
        if (batch.isEmpty()) {
            return;
        }

        try {
            // 批量写入日志
            writeBatch(batch);
            batchWrites.incrementAndGet();
            
            log.debug("Flushed {} audit logs in batch", batch.size());
        } catch (Exception e) {
            failedWrites.incrementAndGet();
            log.error("Failed to write audit log batch", e);
            
            // 失败时降级到同步写入
            for (AuditLogEntry entry : batch) {
                try {
                    syncAuditService.log(entry.getAction(), entry.getMessage());
                } catch (Exception ex) {
                    log.error("Failed to write audit log entry: {}", entry, ex);
                }
            }
        }
    }

    /**
     * 批量写入日志
     */
    private void writeBatch(List<AuditLogEntry> batch) {
        StringBuilder batchLog = new StringBuilder();
        batchLog.append("=== Batch Audit Log ===\n");
        batchLog.append("Timestamp: ").append(LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)).append("\n");
        batchLog.append("Batch Size: ").append(batch.size()).append("\n");
        batchLog.append("Entries:\n");
        
        for (AuditLogEntry entry : batch) {
            batchLog.append(String.format("[%s] %s: %s\n", 
                LocalDateTime.ofInstant(java.time.Instant.ofEpochMilli(entry.getTimestamp()), 
                    java.time.ZoneId.systemDefault()).format(DateTimeFormatter.ISO_LOCAL_DATE_TIME),
                entry.getAction(), 
                entry.getMessage()));
        }
        
        batchLog.append("=== End Batch ===\n");
        
        // 写入日志文件
        log.info(batchLog.toString());
    }

    /**
     * 获取统计信息
     */
    public Map<String, Object> getStats() {
        Map<String, Object> stats = new ConcurrentHashMap<>();
        stats.put("totalLogs", totalLogs.get());
        stats.put("asyncLogs", asyncLogs.get());
        stats.put("syncLogs", syncLogs.get());
        stats.put("batchWrites", batchWrites.get());
        stats.put("failedWrites", failedWrites.get());
        stats.put("queueSize", logQueue.size());
        stats.put("currentBatchSize", currentBatch.get().size());
        stats.put("asyncEnabled", asyncEnabled);
        stats.put("timestamp", System.currentTimeMillis());
        return stats;
    }

    /**
     * 手动刷新批次
     */
    public void flush() {
        flushBatch();
    }

    /**
     * 审计日志条目
     */
    private static class AuditLogEntry {
        private final String action;
        private final String message;
        private final long timestamp;

        public AuditLogEntry(String action, String message, long timestamp) {
            this.action = action;
            this.message = message;
            this.timestamp = timestamp;
        }

        public String getAction() { return action; }
        public String getMessage() { return message; }
        public long getTimestamp() { return timestamp; }

        @Override
        public String toString() {
            return String.format("AuditLogEntry{action='%s', message='%s', timestamp=%d}", 
                    action, message, timestamp);
        }
    }

    /**
     * 同步审计服务（降级使用）
     */
    private static class SyncAuditService {
        public void log(String action, String message) {
            log.info("[AUDIT] {}: {}", action, message);
        }
    }
}
```

### 8.2 配置管理类
```java
@Data
@Configuration
@ConfigurationProperties(prefix = "audit")
public class AuditConfig {

    /**
     * 异步配置
     */
    private Async async = new Async();

    /**
     * 存储配置
     */
    private Storage storage = new Storage();

    /**
     * 异步配置
     */
    @Data
    public static class Async {
        /**
         * 是否启用异步模式
         */
        private boolean enabled = true;

        /**
         * 批次大小
         */
        private int batchSize = 100;

        /**
         * 刷新间隔（毫秒）
         */
        private long flushInterval = 5000;

        /**
         * 队列大小
         */
        private int queueSize = 10000;

        /**
         * 队列超时时间（毫秒）
         */
        private long queueTimeout = 100;

        /**
         * 工作线程数
         */
        private int workerThreads = 1;
    }

    /**
     * 存储配置
     */
    @Data
    public static class Storage {
        /**
         * 存储类型：file, database, elasticsearch, kafka
         */
        private String type = "file";

        /**
         * 文件存储配置
         */
        private File file = new File();

        /**
         * 数据库存储配置
         */
        private Database database = new Database();

        /**
         * Elasticsearch存储配置
         */
        private Elasticsearch elasticsearch = new Elasticsearch();

        /**
         * Kafka存储配置
         */
        private Kafka kafka = new Kafka();
    }

    // ... 其他配置类实现
}
```

### 8.3 监控控制器
```java
@Slf4j
@RestController
@RequestMapping("/api/gateway/audit")
public class AuditController {

    @Autowired
    private AsyncAuditService asyncAuditService;

    /**
     * 获取审计日志服务统计信息
     */
    @GetMapping("/stats")
    public ResponseEntity<Map<String, Object>> getAuditStats() {
        Map<String, Object> stats = asyncAuditService.getStats();
        log.debug("Audit stats requested: {}", stats);
        return ResponseEntity.ok(stats);
    }

    /**
     * 手动刷新审计日志批次
     */
    @PostMapping("/flush")
    public ResponseEntity<Map<String, Object>> flushAuditLogs() {
        long startTime = System.currentTimeMillis();
        asyncAuditService.flush();
        long duration = System.currentTimeMillis() - startTime;
        
        Map<String, Object> result = Map.of(
            "status", "success",
            "message", "Audit logs flushed successfully",
            "duration", duration + "ms",
            "timestamp", System.currentTimeMillis()
        );
        
        log.info("Manual audit log flush completed in {}ms", duration);
        return ResponseEntity.ok(result);
    }

    /**
     * 测试审计日志记录
     */
    @PostMapping("/test")
    public ResponseEntity<Map<String, Object>> testAuditLog(@RequestParam String action, 
                                                           @RequestParam String message) {
        long startTime = System.currentTimeMillis();
        asyncAuditService.log(action, message);
        long duration = System.currentTimeMillis() - startTime;
        
        Map<String, Object> result = Map.of(
            "status", "success",
            "message", "Test audit log recorded",
            "action", action,
            "message", message,
            "duration", duration + "ms",
            "timestamp", System.currentTimeMillis()
        );
        
        log.info("Test audit log recorded - Action: {}, Message: {}, Duration: {}ms", 
                action, message, duration);
        return ResponseEntity.ok(result);
    }

    /**
     * 批量测试审计日志记录
     */
    @PostMapping("/test/batch")
    public ResponseEntity<Map<String, Object>> testBatchAuditLog(@RequestParam int count) {
        long startTime = System.currentTimeMillis();
        
        for (int i = 0; i < count; i++) {
            asyncAuditService.log("test_batch", "Test message " + (i + 1));
        }
        
        long duration = System.currentTimeMillis() - startTime;
        
        Map<String, Object> result = Map.of(
            "status", "success",
            "message", "Batch test audit logs recorded",
            "count", count,
            "duration", duration + "ms",
            "avgDuration", String.format("%.2f", (double) duration / count) + "ms",
            "timestamp", System.currentTimeMillis()
        );
        
        log.info("Batch test audit logs recorded - Count: {}, Duration: {}ms, Avg: {}ms", 
                count, duration, String.format("%.2f", (double) duration / count));
        return ResponseEntity.ok(result);
    }

    /**
     * 获取审计日志服务健康状态
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, Object>> getAuditHealth() {
        Map<String, Object> stats = asyncAuditService.getStats();
        
        Map<String, Object> health = Map.of(
            "status", "UP",
            "asyncEnabled", stats.get("asyncEnabled"),
            "queueSize", stats.get("queueSize"),
            "currentBatchSize", stats.get("currentBatchSize"),
            "totalLogs", stats.get("totalLogs"),
            "asyncLogs", stats.get("asyncLogs"),
            "syncLogs", stats.get("syncLogs"),
            "batchWrites", stats.get("batchWrites"),
            "failedWrites", stats.get("failedWrites"),
            "timestamp", System.currentTimeMillis()
        );
        
        log.debug("Audit health check: {}", health);
        return ResponseEntity.ok(health);
    }

    /**
     * 获取审计日志服务配置信息
     */
    @GetMapping("/config")
    public ResponseEntity<Map<String, Object>> getAuditConfig() {
        Map<String, Object> config = Map.of(
            "asyncEnabled", true,
            "batchSize", 100,
            "flushInterval", "5000ms",
            "queueSize", 10000,
            "queueTimeout", "100ms",
            "workerThreads", 1,
            "timestamp", System.currentTimeMillis()
        );
        
        log.debug("Audit config requested: {}", config);
        return ResponseEntity.ok(config);
    }
}
```

## 9. 性能优化建议

### 9.1 内存优化
- 合理设置队列大小，避免内存溢出
- 定期清理过期的日志条目
- 使用对象池减少GC压力

### 9.2 I/O优化
- 使用缓冲写入减少磁盘I/O
- 批量写入提升吞吐量
- 异步I/O操作避免阻塞

### 9.3 线程优化
- 合理设置工作线程数
- 避免线程竞争和锁竞争
- 使用无锁数据结构提升并发性能

## 10. 故障处理策略

### 10.1 队列满处理
- 降级到同步写入
- 丢弃部分日志（可配置）
- 动态调整批次大小

### 10.2 存储故障处理
- 本地缓存备份
- 重试机制
- 故障转移

### 10.3 服务重启处理
- 优雅关闭时刷新剩余日志
- 启动时恢复状态
- 数据一致性保证

## 11. 总结

这种设计完美平衡了性能、可靠性和可维护性，是现代分布式系统日志记录的最佳实践之一。通过异步批量写入，可以显著提升系统的日志记录性能，同时保证数据的可靠性和完整性。 